{"cells": [{"cell_type": "code", "metadata": {}, "source": ["# ===============================\n", "# INSTALLERA BIBLIOTEK\n", "# ===============================\n", "!pip install googletrans==4.0.0rc1 python-docx requests beautifulsoup4\n", "\n", "from google.colab import drive\n", "import os\n", "import time\n", "import requests\n", "from bs4 import BeautifulSoup\n", "from googletrans import Translator\n", "from docx import Document\n", "from datetime import datetime\n", "\n", "# ===============================\n", "# MONTERA GOOGLE DRIVE\n", "# ===============================\n", "drive.mount('/content/drive')\n", "\n", "BASE_DIR = \"/content/drive/MyDrive/CPS_Artiklar_Pashto\"\n", "os.makedirs(BASE_DIR, exist_ok=True)\n", "LOG_FILE = os.path.join(BASE_DIR, \"oversatta_logg.txt\")\n", "\n", "# ===============================\n", "# HJ\u00c4LPFUNKTIONER\n", "# ===============================\n", "def load_log():\n", "    if not os.path.exists(LOG_FILE):\n", "        return set()\n", "    with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n", "        return set(line.strip().split(\"\\t\")[0] for line in f if line.strip())\n", "\n", "def update_log(url):\n", "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n", "        now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n", "        f.write(f\"{url}\\t{now}\\n\")\n", "\n", "def fetch_soup(url):\n", "    try:\n", "        r = requests.get(url, timeout=15)\n", "        r.raise_for_status()\n", "        return BeautifulSoup(r.text, \"html.parser\")\n", "    except Exception as e:\n", "        print(f\"\u274c Fel vid h\u00e4mtning av {url}: {e}\")\n", "        return None\n", "\n", "def get_all_article_links():\n", "    base = \"https://www.cpsglobal.org/articles\"\n", "    links = set()\n", "    page = 0\n", "    while True:\n", "        url = base if page == 0 else f\"{base}?page={page}\"\n", "        soup = fetch_soup(url)\n", "        if not soup:\n", "            break\n", "        page_links = [\n", "            a[\"href\"] for a in soup.select(\"div.view-content a\") if a.get(\"href\", \"\").startswith(\"/articles/\")\n", "        ]\n", "        if not page_links:\n", "            break\n", "        for pl in page_links:\n", "            links.add(\"https://www.cpsglobal.org\" + pl)\n", "        page += 1\n", "        time.sleep(0.5)\n", "    return sorted(links)\n", "\n", "def extract_title_and_paragraphs(article_url: str):\n", "    soup = fetch_soup(article_url)\n", "    if not soup:\n", "        return \"\", []\n", "    h1 = soup.find(\"h1\")\n", "    title = h1.get_text(strip=True) if h1 else \"Artikel\"\n", "    candidates = []\n", "    scope = soup.find(\"article\") or soup.find(\"main\")\n", "    if scope:\n", "        for p in scope.find_all(\"p\"):\n", "            txt = p.get_text(\" \", strip=True)\n", "            if txt:\n", "                candidates.append(txt)\n", "    if not candidates:\n", "        for p in soup.find_all(\"p\"):\n", "            txt = p.get_text(\" \", strip=True)\n", "            if txt:\n", "                candidates.append(txt)\n", "    blacklist_snips = [\"Subscribe\",\"Stay informed\",\"newsletter\",\"Donate\",\"About Us\",\"FOLLOW US\",\"Share\",\"\u00a9\",\"Powered by\",\"CPS shares spiritual wisdom\"]\n", "    clean = []\n", "    for t in candidates:\n", "        if any(b.lower() in t.lower() for b in blacklist_snips):\n", "            continue\n", "        if len(t) < 20 and not t.endswith(\".\"):\n", "            continue\n", "        clean.append(t)\n", "    if clean and title and clean[0].strip().lower() == title.strip().lower():\n", "        clean = clean[1:]\n", "    return title or \"Artikel\", clean\n", "\n", "translator = Translator()\n", "\n", "def oversatt_och_spara(url, artikelnummer):\n", "    try:\n", "        title, paragraphs = extract_title_and_paragraphs(url)\n", "        if not paragraphs:\n", "            print(f\"\u26a0\ufe0f Ingen text hittad i {url}\")\n", "            return None\n", "        doc = Document()\n", "        doc.add_heading(title, 0)\n", "        for para in paragraphs:\n", "            try:\n", "                ps = translator.translate(para, src=\"en\", dest=\"ps\").text\n", "            except Exception:\n", "                ps = para\n", "            doc.add_paragraph(ps)\n", "            time.sleep(2)\n", "        safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:50]\n", "        filename = os.path.join(BASE_DIR, f\"{artikelnummer:04d}_{safe_title}.docx\")\n", "        doc.save(filename)\n", "        update_log(url)\n", "        print(f\"\ud83d\udcdd Sparad: {filename}\")\n", "        return filename\n", "    except Exception as e:\n", "        print(f\"\u274c Fel i oversatt_och_spara: {e}\")\n", "        return None\n", "\n", "alla = get_all_article_links()\n", "done = load_log()\n", "kvar = [u for u in alla if u not in done]\n", "\n", "print(f\"Totalt hittade: {len(alla)} artiklar\")\n", "print(f\"\ud83d\udccc Redan \u00f6versatta: {len(done)}\")\n", "print(f\"\ud83e\uddee \u00c5terst\u00e5r: {len(kvar)}\")\n", "\n", "MAX = 2\n", "for i, url in enumerate(kvar[:MAX], start=1):\n", "    oversatt_och_spara(url, i)\n", "    time.sleep(5)\n", "\n", "print(\"\ud83d\ude80 Klart!\")\n"], "outputs": [], "execution_count": null}], "metadata": {"colab": {"name": "maulana_cps_pashto_translate"}}, "nbformat": 4, "nbformat_minor": 2}
